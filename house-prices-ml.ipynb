{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":5407,"databundleVersionId":868283,"isSourceIdPinned":false,"sourceType":"competition"},{"sourceId":14819625,"sourceType":"datasetVersion","datasetId":9477301}],"dockerImageVersionId":31259,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# ML workflow for the house prices dataset\n\nThis script contains a machine learning workflow for the house prices dataset. The workflow includes ...","metadata":{}},{"cell_type":"code","source":"# load libraries\nimport pandas as pd\nimport numpy as np\nimport pickle\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler, QuantileTransformer\nfrom sklearn.compose import ColumnTransformer\nfrom xgboost import XGBRegressor\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import GridSearchCV\n\n\n# Print file names in directory\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-13T12:06:57.142358Z","iopub.execute_input":"2026-02-13T12:06:57.143035Z","iopub.status.idle":"2026-02-13T12:06:57.154606Z","shell.execute_reply.started":"2026-02-13T12:06:57.143003Z","shell.execute_reply":"2026-02-13T12:06:57.153561Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/competitions/house-prices-advanced-regression-techniques/sample_submission.csv\n/kaggle/input/competitions/house-prices-advanced-regression-techniques/data_description.txt\n/kaggle/input/competitions/house-prices-advanced-regression-techniques/train.csv\n/kaggle/input/competitions/house-prices-advanced-regression-techniques/test.csv\n/kaggle/input/eda-results/eda_config.pickle\n","output_type":"stream"}],"execution_count":50},{"cell_type":"code","source":"# define necessary functions\ndef encode_ordinal(df, columns, order):\n    for column in columns:\n        df[column] = (\n            pd.Categorical(df[column], categories=order, ordered=True)\n            .codes\n            .astype('float')\n        )\n        df[column] = df[column].replace(-1, np.nan)\n    return df\n\n# define function to run parameter grid search together with cross-validation\n# important to specify scoring to negative RMSE (in line with kaggle)\ndef run_grid_search(estimator, param_grid, preprocessor, X, y, cv, scoring=\"neg_root_mean_squared_error\"):\n    \n    # define pipeline\n    pipe = Pipeline([\n        ('preprocessor', preprocessor),\n        ('model', estimator)\n    ])\n\n    # set-up grid search\n    grid = GridSearchCV(\n        estimator=pipe,\n        param_grid=param_grid,\n        cv=cv, # k-fold variable must be defined before so random state is always the same\n        scoring=scoring,\n        return_train_score=True\n    )\n\n    # run grid search\n    grid.fit(X, y)\n\n    # Extract results across runs (this runs independent of how many parameters are specified)\n    results_df = pd.DataFrame({\n        key.replace(\"param_model__\", \"\"): grid.cv_results_[key]\n        for key in grid.cv_results_.keys()\n        if key.startswith(\"param_model__\")\n    })\n\n    results_df[\"mean_rmse\"] = -grid.cv_results_[\"mean_test_score\"]  # Negate so that the final score is shown as positive\n    results_df[\"std_rmse\"] = grid.cv_results_[\"std_test_score\"]\n\n    return grid, results_df.sort_values(\"mean_rmse\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-13T12:06:57.173673Z","iopub.execute_input":"2026-02-13T12:06:57.174621Z","iopub.status.idle":"2026-02-13T12:06:57.183304Z","shell.execute_reply.started":"2026-02-13T12:06:57.174573Z","shell.execute_reply":"2026-02-13T12:06:57.182333Z"}},"outputs":[],"execution_count":51},{"cell_type":"markdown","source":"## Step 1: Load data and EDA results","metadata":{}},{"cell_type":"code","source":"# specify input paths\ntrain_file_path = '/kaggle/input/competitions/house-prices-advanced-regression-techniques/train.csv'\ntest_file_path = '/kaggle/input/competitions/house-prices-advanced-regression-techniques/test.csv'\nsubmission_file_path = '/kaggle/input/competitions/house-prices-advanced-regression-techniques/sample_submission.csv'\neda_config_file_path = '/kaggle/input/eda-results/eda_config.pickle'\n\n# load as pandas data frames\ndf = pd.read_csv(train_file_path)\ndf_test = pd.read_csv(test_file_path)\nsub_df = pd.read_csv(submission_file_path)\n\n# load eda config file\nwith open(eda_config_file_path, 'rb') as handle:\n    eda = pickle.load(handle)\n\n# separate variables\ntarget = df[\"SalePrice\"]\ndf = df.drop(columns=[\"SalePrice\"])\n\n# display options\npd.options.display.max_columns = 50\npd.options.display.max_rows = 50","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-13T12:06:57.184967Z","iopub.execute_input":"2026-02-13T12:06:57.185312Z","iopub.status.idle":"2026-02-13T12:06:57.248131Z","shell.execute_reply.started":"2026-02-13T12:06:57.185288Z","shell.execute_reply":"2026-02-13T12:06:57.247336Z"}},"outputs":[],"execution_count":52},{"cell_type":"code","source":"# show dict keys of eda results\nfor i in eda.keys():\n    print(i)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-13T12:06:57.249571Z","iopub.execute_input":"2026-02-13T12:06:57.249946Z","iopub.status.idle":"2026-02-13T12:06:57.255325Z","shell.execute_reply.started":"2026-02-13T12:06:57.249921Z","shell.execute_reply":"2026-02-13T12:06:57.254328Z"}},"outputs":[{"name":"stdout","text":"dropped_columns\nordinal_maps\ndtype_overrides\ncategory_missing_levels\nfinal_features\nnumeric_features\ncategorical_features\nrare_category_features\nskewed_features\noutlier_sensitive_features\ntarget_transform\nnumeric_predictors_correlation\ncategorical_predictors_etasquared\nhigh_corr_pairs_numeric\nhigh_association_pairs_categorical\n","output_type":"stream"}],"execution_count":53},{"cell_type":"markdown","source":"First, the dataframe will be corrected based on the results of the exploratory data analysis.","metadata":{}},{"cell_type":"code","source":"# recode into ordinal variables\nfor col, mapping in eda['ordinal_maps'].items():\n    df = encode_ordinal(df, [col], mapping)\n    df_test = encode_ordinal(df_test, [col], mapping)\n\n# fix dtype\nfor col, dtype in eda['dtype_overrides'].items():\n    df[col] = df[col].astype(dtype)\n    df_test[col] = df_test[col].astype(dtype)\n\n# log-transform target\ntarget = np.log1p(target) # log-transformation takes care of the usual right-skew of price distributions","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-13T12:06:57.256387Z","iopub.execute_input":"2026-02-13T12:06:57.256780Z","iopub.status.idle":"2026-02-13T12:06:57.293678Z","shell.execute_reply.started":"2026-02-13T12:06:57.256756Z","shell.execute_reply":"2026-02-13T12:06:57.292823Z"}},"outputs":[],"execution_count":54},{"cell_type":"markdown","source":"## Step 2: Pipeline set-up","metadata":{}},{"cell_type":"markdown","source":"The EDA previously identified several skewed numeric variables with outliers. Categorical variables were also shown to have some rare categories. The preprocessing pipeline can take this into account by scaling numeric variables and combining infrequent categories. Collapsing infrequent categories has the benefit of reducing the required columns for one-hot-encoding.","metadata":{}},{"cell_type":"code","source":"# split into numeric and categorical\nnumeric_features = eda['numeric_features']\ncategorical_features = eda['categorical_features']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-13T12:06:57.295759Z","iopub.execute_input":"2026-02-13T12:06:57.296004Z","iopub.status.idle":"2026-02-13T12:06:57.300280Z","shell.execute_reply.started":"2026-02-13T12:06:57.295981Z","shell.execute_reply":"2026-02-13T12:06:57.299428Z"}},"outputs":[],"execution_count":55},{"cell_type":"code","source":"# preprocessing numerical features (insert median for missing values and use z-score scaling)\nnumeric_transformer = Pipeline(steps = [\n    ('imputer', SimpleImputer(strategy = 'median')), # impute the median for missing values\n    ('scaler',  QuantileTransformer(output_distribution='normal', n_quantiles=1000)) # this ensures outliers and skew are minimized\n])\n\n# preprocessing categorical features (insert most frequent for missing values)\ncategorical_transformer = Pipeline(steps = [\n    ('imputer', SimpleImputer(strategy = 'most_frequent')), # impute most frequent category if missing\n    ('onehot', OneHotEncoder( # this is set up so that rare categories are grouped into a 'other' category\n        handle_unknown = 'infrequent_if_exist',\n        min_frequency = 0.01)) # this takes care of rare categories that appear less than 1%\n])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-13T12:06:57.301326Z","iopub.execute_input":"2026-02-13T12:06:57.301687Z","iopub.status.idle":"2026-02-13T12:06:57.317658Z","shell.execute_reply.started":"2026-02-13T12:06:57.301648Z","shell.execute_reply":"2026-02-13T12:06:57.316647Z"}},"outputs":[],"execution_count":56},{"cell_type":"code","source":"# define the columns transformer\npreprocessor = ColumnTransformer(transformers = [\n    ('num', numeric_transformer, numeric_features),\n    ('cat', categorical_transformer, categorical_features)\n])\n\n# prepare the train and test datasets\nX = df[numeric_features + categorical_features]\nX_test = df_test[numeric_features + categorical_features]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-13T12:06:57.318853Z","iopub.execute_input":"2026-02-13T12:06:57.319189Z","iopub.status.idle":"2026-02-13T12:06:57.342998Z","shell.execute_reply.started":"2026-02-13T12:06:57.319149Z","shell.execute_reply":"2026-02-13T12:06:57.342161Z"}},"outputs":[],"execution_count":57},{"cell_type":"markdown","source":"## Step 3: Model training and cross-validation","metadata":{}},{"cell_type":"markdown","source":"The EDA has shown that there are a few pairs of variables with high correlation. Therefore, I will first use tree-based models since they are robust to multicollinearity.","metadata":{}},{"cell_type":"code","source":"# set random seed\nrs = 42\nnp.random.seed(rs)\n\n# define kfold split\nkf = KFold(n_splits=5, shuffle=True, random_state=rs)\n\nparam_grid = {\n    'model__max_depth': [3, 6, 9, 12],\n    'model__n_estimators': [50, 100, 200, 250],\n    'model__learning_rate': [0.05, 0.1]\n}\n\nestimator = XGBRegressor(random_state=rs)\n\ngrid_xgb, results_xgb = run_grid_search(\n    estimator, param_grid, preprocessor, X, target, kf,\n    scoring=\"neg_root_mean_squared_error\"\n)\nresults_xgb.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-13T12:06:57.344214Z","iopub.execute_input":"2026-02-13T12:06:57.344630Z","iopub.status.idle":"2026-02-13T12:10:00.903372Z","shell.execute_reply.started":"2026-02-13T12:06:57.344588Z","shell.execute_reply":"2026-02-13T12:10:00.902643Z"}},"outputs":[{"execution_count":58,"output_type":"execute_result","data":{"text/plain":"    learning_rate  max_depth  n_estimators  mean_rmse  std_rmse\n19           0.10          3           250   0.131301  0.016746\n18           0.10          3           200   0.131724  0.017629\n3            0.05          3           250   0.132043  0.018642\n2            0.05          3           200   0.133182  0.018639\n17           0.10          3           100   0.133728  0.017973","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>learning_rate</th>\n      <th>max_depth</th>\n      <th>n_estimators</th>\n      <th>mean_rmse</th>\n      <th>std_rmse</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>19</th>\n      <td>0.10</td>\n      <td>3</td>\n      <td>250</td>\n      <td>0.131301</td>\n      <td>0.016746</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>0.10</td>\n      <td>3</td>\n      <td>200</td>\n      <td>0.131724</td>\n      <td>0.017629</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.05</td>\n      <td>3</td>\n      <td>250</td>\n      <td>0.132043</td>\n      <td>0.018642</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.05</td>\n      <td>3</td>\n      <td>200</td>\n      <td>0.133182</td>\n      <td>0.018639</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>0.10</td>\n      <td>3</td>\n      <td>100</td>\n      <td>0.133728</td>\n      <td>0.017973</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":58},{"cell_type":"code","source":"# get the parameters with best RMSE estimate\nbest_max_depth = results_xgb['max_depth'].iloc[0]\nbest_n_estimators = results_xgb['n_estimators'].iloc[0]\nbest_learning_rate = results_xgb['learning_rate'].iloc[0]\n\n# define the pipeline and model with these parameters\nmodel = Pipeline(steps = [\n    ('preprocessor', preprocessor),\n    ('model', XGBRegressor(\n        random_state=rs,\n        max_depth=best_max_depth,\n        n_estimators=best_n_estimators,\n        learning_rate=best_learning_rate\n    ))\n])\n\n# train the model and predict test data\nmodel.fit(X,target)\ntarget_pred = model.predict(X_test)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-13T12:10:00.904180Z","iopub.execute_input":"2026-02-13T12:10:00.904411Z","iopub.status.idle":"2026-02-13T12:10:01.474780Z","shell.execute_reply.started":"2026-02-13T12:10:00.904387Z","shell.execute_reply":"2026-02-13T12:10:01.474140Z"}},"outputs":[],"execution_count":59},{"cell_type":"code","source":"# create results dataframe\nsubmission = pd.DataFrame({\n    'Id': df_test['Id'],\n    'SalePrice': np.expm1(target_pred) # take inverse of logarithm for kaggle competition\n})\n\n# save in .csv format\nsubmission.to_csv(\"submission_XGBRegressor.csv\", index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-13T12:10:01.475838Z","iopub.execute_input":"2026-02-13T12:10:01.476230Z","iopub.status.idle":"2026-02-13T12:10:01.487065Z","shell.execute_reply.started":"2026-02-13T12:10:01.476204Z","shell.execute_reply":"2026-02-13T12:10:01.486367Z"}},"outputs":[],"execution_count":60},{"cell_type":"markdown","source":"The basic XGB Regressor has a test RMSE of 0.133 (kaggle leaderboard). This estimate is very similar to the cross-validated RMSE, so no overfitting. However, there's several things left to do to try and increase the accuracy of the predictions. ","metadata":{}},{"cell_type":"markdown","source":"## To-do\n\n- Extract feature importance\n- Add feature engineering\n- Use ensemble learning","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}